{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "I = 4  # Input dimensions\n",
    "H = 64  # Hidden layer dimensions\n",
    "O = 2  # output dimensions (one-hot encoding)\n",
    "LEARNING_RATE = 0.00001\n",
    "REPLAY_LENGTH = 1000\n",
    "EPISODE_NUM = 1000\n",
    "EPISODE_LENGTH = 200\n",
    "WARMUP_LENGTH = 10\n",
    "EPSILON = 0.1\n",
    "MINIBATCH_SIZE = 1000\n",
    "# BATCH_SIZE =\n",
    "EPOCHS = 1\n",
    "GAMMA = 1\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(I,H)\n",
    "        self.lin2 = nn.Linear(H,H)\n",
    "        self.lin3 = nn.Linear(H,O)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = F.relu(self.lin1(xb))\n",
    "        xb = F.relu(self.lin2(xb))\n",
    "        return self.lin3(xb)\n",
    "\n",
    "def fit(epochs, batch_size, lr, model, loss_func, x, y):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_num = x.size()[0] // batch_size\n",
    "        for i in range(batch_num):\n",
    "            xb = x[i*batch_size:(i+1)*batch_size]\n",
    "            yb = y[i*batch_size:(i+1)*batch_size]\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "        print(f'Epoch {epoch}: Loss: {loss}')\n",
    "\n",
    "def loss_func(y1, y2):\n",
    "    return torch.sum(torch.square(y1-y2))\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, n):\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.n = n\n",
    "        self.arr = []\n",
    "\n",
    "    def debug(self, n):\n",
    "        print(np.array(self.arr, dtype='object'))\n",
    "        print(self.rng.choice(np.array(self.arr, dtype='object'), (n,)))\n",
    "\n",
    "    def sample(self, n):\n",
    "        minibatch = self.rng.choice(np.array(self.arr, dtype='object'), (n,))\n",
    "        obs2_arr = torch.from_numpy(np.vstack(minibatch[:, 3]).astype(np.float))\n",
    "        obs_arr = torch.from_numpy(np.vstack(minibatch[:, 0]).astype(np.float))\n",
    "        action_arr = torch.from_numpy(np.vstack(minibatch[:, 1]).astype(np.int).flatten())\n",
    "        reward_arr = torch.from_numpy(np.vstack(minibatch[:, 2]).astype(np.float).flatten())\n",
    "        return obs_arr, action_arr, reward_arr, obs2_arr\n",
    "\n",
    "    def add_one(self, v):\n",
    "        self.arr.append(v)\n",
    "        if len(self.arr) > self.n:\n",
    "            self.arr.pop(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = DQN().to(dev)\n",
    "replay = Replay(REPLAY_LENGTH)\n",
    "env = gym.make('CartPole-v0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\leeji\\pycharmprojects\\algorithms\\venv\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001B[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001B[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 570.0047607421875\n",
      "Epoch 0: Loss: 177.67340087890625\n",
      "Epoch 0: Loss: 350.0753173828125\n",
      "Epoch 0: Loss: 315.1231994628906\n",
      "Epoch 0: Loss: 241.1131591796875\n",
      "Epoch 0: Loss: 101.14593505859375\n",
      "Epoch 0: Loss: 264.685302734375\n",
      "Epoch 0: Loss: 1692.475341796875\n",
      "Epoch 0: Loss: 57394.3828125\n",
      "Epoch 0: Loss: 37974.67578125\n",
      "Epoch 0: Loss: 111.63449096679688\n",
      "Epoch 0: Loss: 111.12545776367188\n",
      "Epoch 0: Loss: 111.80415344238281\n",
      "Epoch 0: Loss: 136.90884399414062\n",
      "Epoch 0: Loss: 166.64620971679688\n",
      "Epoch 0: Loss: 150.34725952148438\n",
      "Epoch 0: Loss: 168.26280212402344\n",
      "Epoch 0: Loss: 192.6641845703125\n",
      "Epoch 0: Loss: 198.60406494140625\n",
      "Epoch 0: Loss: 168.82664489746094\n",
      "Epoch 0: Loss: 137.48690795898438\n",
      "Epoch 0: Loss: 130.38595581054688\n",
      "Epoch 0: Loss: 116.85819244384766\n",
      "Epoch 0: Loss: 126.87364196777344\n",
      "Epoch 0: Loss: 151.2644500732422\n",
      "Epoch 0: Loss: 158.30499267578125\n",
      "Epoch 0: Loss: 161.955078125\n",
      "Epoch 0: Loss: 137.8616943359375\n",
      "Epoch 0: Loss: 147.82733154296875\n",
      "Epoch 0: Loss: 167.44342041015625\n",
      "Epoch 0: Loss: 137.09117126464844\n",
      "Epoch 0: Loss: 127.08690643310547\n",
      "Epoch 0: Loss: 151.5906982421875\n",
      "Epoch 0: Loss: 134.77212524414062\n",
      "Epoch 0: Loss: 138.10501098632812\n",
      "Epoch 0: Loss: 162.41677856445312\n",
      "Epoch 0: Loss: 128.88583374023438\n",
      "Epoch 0: Loss: 123.99864196777344\n",
      "Epoch 0: Loss: 120.25965118408203\n",
      "Epoch 0: Loss: 142.7605743408203\n",
      "Epoch 0: Loss: 117.9381332397461\n",
      "Epoch 0: Loss: 114.65960693359375\n",
      "Epoch 0: Loss: 139.8164520263672\n",
      "Epoch 0: Loss: 139.53927612304688\n",
      "Epoch 0: Loss: 132.98687744140625\n",
      "Epoch 0: Loss: 117.0122299194336\n",
      "Epoch 0: Loss: 125.89704895019531\n",
      "Epoch 0: Loss: 132.2812957763672\n",
      "Epoch 0: Loss: 104.48410034179688\n",
      "Epoch 0: Loss: 107.86369323730469\n",
      "Epoch 0: Loss: 105.0904541015625\n",
      "Epoch 0: Loss: 120.43013000488281\n",
      "Epoch 0: Loss: 105.55439758300781\n",
      "Epoch 0: Loss: 114.1202163696289\n",
      "Epoch 0: Loss: 131.24020385742188\n",
      "Epoch 0: Loss: 132.7353515625\n",
      "Epoch 0: Loss: 128.17027282714844\n",
      "Epoch 0: Loss: 96.96887969970703\n",
      "Epoch 0: Loss: 96.56666564941406\n",
      "Epoch 0: Loss: 101.3239974975586\n",
      "Epoch 0: Loss: 114.99606323242188\n",
      "Epoch 0: Loss: 115.49650573730469\n",
      "Epoch 0: Loss: 155.5978546142578\n",
      "Epoch 0: Loss: 156.447265625\n",
      "Epoch 0: Loss: 153.0154571533203\n",
      "Epoch 0: Loss: 112.3100357055664\n",
      "Epoch 0: Loss: 107.50306701660156\n",
      "Epoch 0: Loss: 116.11475372314453\n",
      "Epoch 0: Loss: 120.06922912597656\n",
      "Epoch 0: Loss: 115.73102569580078\n",
      "Epoch 0: Loss: 122.8006362915039\n",
      "Epoch 0: Loss: 112.24253845214844\n",
      "Epoch 0: Loss: 128.83126831054688\n",
      "Epoch 0: Loss: 121.70702362060547\n",
      "Epoch 0: Loss: 124.76324462890625\n",
      "Epoch 0: Loss: 107.72588348388672\n",
      "Epoch 0: Loss: 114.54634857177734\n",
      "Epoch 0: Loss: 114.46383666992188\n",
      "Epoch 0: Loss: 104.65399932861328\n",
      "Epoch 0: Loss: 118.11700439453125\n",
      "Epoch 0: Loss: 116.37686920166016\n",
      "Epoch 0: Loss: 104.18251037597656\n",
      "Epoch 0: Loss: 108.22280883789062\n",
      "Epoch 0: Loss: 126.1364974975586\n",
      "Epoch 0: Loss: 109.40330505371094\n",
      "Epoch 0: Loss: 101.28842163085938\n",
      "Epoch 0: Loss: 120.59793853759766\n",
      "Epoch 0: Loss: 110.37124633789062\n",
      "Epoch 0: Loss: 93.5084228515625\n",
      "Epoch 0: Loss: 94.50349426269531\n",
      "Epoch 0: Loss: 114.09346008300781\n",
      "Epoch 0: Loss: 96.71298217773438\n",
      "Epoch 0: Loss: 94.13217163085938\n",
      "Epoch 0: Loss: 84.09344482421875\n",
      "Epoch 0: Loss: 74.71687316894531\n",
      "Epoch 0: Loss: 65.69757080078125\n",
      "Epoch 0: Loss: 62.599708557128906\n",
      "Epoch 0: Loss: 55.718727111816406\n",
      "Epoch 0: Loss: 59.873435974121094\n",
      "Epoch 0: Loss: 57.28386306762695\n",
      "Epoch 0: Loss: 54.99210739135742\n",
      "Epoch 0: Loss: 51.67409133911133\n",
      "Epoch 0: Loss: 58.841224670410156\n",
      "Epoch 0: Loss: 67.91017150878906\n",
      "Epoch 0: Loss: 66.3416748046875\n",
      "Epoch 0: Loss: 112.96228790283203\n",
      "Epoch 0: Loss: 80.60987854003906\n",
      "Epoch 0: Loss: 102.02162170410156\n",
      "Epoch 0: Loss: 91.84822082519531\n",
      "Epoch 0: Loss: 74.63188934326172\n",
      "Epoch 0: Loss: 84.13800048828125\n",
      "Epoch 0: Loss: 80.43962097167969\n",
      "Epoch 0: Loss: 91.84201049804688\n",
      "Epoch 0: Loss: 77.77685546875\n",
      "Epoch 0: Loss: 88.7911376953125\n",
      "Epoch 0: Loss: 60.4166259765625\n",
      "Epoch 0: Loss: 52.53214645385742\n",
      "Epoch 0: Loss: 48.67087936401367\n",
      "Epoch 0: Loss: 47.7119026184082\n",
      "Epoch 0: Loss: 73.70321655273438\n",
      "Epoch 0: Loss: 84.98843383789062\n",
      "Epoch 0: Loss: 81.79493713378906\n",
      "Epoch 0: Loss: 94.08686065673828\n",
      "Epoch 0: Loss: 112.87889099121094\n",
      "Epoch 0: Loss: 84.96546936035156\n",
      "Epoch 0: Loss: 75.88442993164062\n",
      "Epoch 0: Loss: 92.08555603027344\n",
      "Epoch 0: Loss: 59.760398864746094\n",
      "Epoch 0: Loss: 47.53607940673828\n",
      "Epoch 0: Loss: 37.640655517578125\n",
      "Epoch 0: Loss: 48.62593078613281\n",
      "Epoch 0: Loss: 49.80763244628906\n",
      "Epoch 0: Loss: 54.66082000732422\n",
      "Epoch 0: Loss: 42.686492919921875\n",
      "Epoch 0: Loss: 42.52732849121094\n",
      "Epoch 0: Loss: 38.50675582885742\n",
      "Epoch 0: Loss: 54.602989196777344\n",
      "Epoch 0: Loss: 50.58930587768555\n",
      "Epoch 0: Loss: 78.55320739746094\n",
      "Epoch 0: Loss: 74.64667510986328\n",
      "Epoch 0: Loss: 87.58585357666016\n",
      "Epoch 0: Loss: 93.64488220214844\n",
      "Epoch 0: Loss: 96.25668334960938\n",
      "Epoch 0: Loss: 85.5787124633789\n",
      "Epoch 0: Loss: 96.7255859375\n",
      "Epoch 0: Loss: 86.38667297363281\n",
      "Epoch 0: Loss: 75.56236267089844\n",
      "Epoch 0: Loss: 62.5969352722168\n",
      "Epoch 0: Loss: 57.54268264770508\n",
      "Epoch 0: Loss: 57.71582794189453\n",
      "Epoch 0: Loss: 59.695526123046875\n",
      "Epoch 0: Loss: 85.19825744628906\n",
      "Epoch 0: Loss: 71.1865005493164\n",
      "Epoch 0: Loss: 58.447669982910156\n",
      "Epoch 0: Loss: 57.61506652832031\n",
      "Epoch 0: Loss: 64.45509338378906\n",
      "Epoch 0: Loss: 54.443458557128906\n",
      "Epoch 0: Loss: 56.372779846191406\n",
      "Epoch 0: Loss: 83.26919555664062\n",
      "Epoch 0: Loss: 96.48997497558594\n",
      "Epoch 0: Loss: 64.59608459472656\n",
      "Epoch 0: Loss: 58.329322814941406\n",
      "Epoch 0: Loss: 59.826690673828125\n",
      "Epoch 0: Loss: 58.802040100097656\n",
      "Epoch 0: Loss: 53.698219299316406\n",
      "Epoch 0: Loss: 57.87603759765625\n",
      "Epoch 0: Loss: 62.421905517578125\n",
      "Epoch 0: Loss: 74.19295501708984\n",
      "Epoch 0: Loss: 85.42860412597656\n",
      "Epoch 0: Loss: 104.67172241210938\n",
      "Epoch 0: Loss: 72.8792724609375\n",
      "Epoch 0: Loss: 91.6899185180664\n",
      "Epoch 0: Loss: 97.43678283691406\n",
      "Epoch 0: Loss: 89.38482666015625\n",
      "Epoch 0: Loss: 100.3096923828125\n",
      "Epoch 0: Loss: 79.57072448730469\n",
      "Epoch 0: Loss: 66.7901382446289\n",
      "Epoch 0: Loss: 57.51361846923828\n",
      "Epoch 0: Loss: 37.435569763183594\n",
      "Epoch 0: Loss: 55.36200714111328\n",
      "Epoch 0: Loss: 70.37090301513672\n",
      "Epoch 0: Loss: 62.1605110168457\n",
      "Epoch 0: Loss: 75.67733764648438\n",
      "Epoch 0: Loss: 77.61183166503906\n",
      "Epoch 0: Loss: 86.42460632324219\n",
      "Epoch 0: Loss: 71.47494506835938\n",
      "Epoch 0: Loss: 82.37120056152344\n",
      "Epoch 0: Loss: 65.45747375488281\n",
      "Epoch 0: Loss: 73.24592590332031\n",
      "Epoch 0: Loss: 51.925376892089844\n",
      "Epoch 0: Loss: 65.29853057861328\n",
      "Epoch 0: Loss: 52.39421844482422\n",
      "Epoch 0: Loss: 61.760345458984375\n",
      "Epoch 0: Loss: 76.84740447998047\n",
      "Epoch 0: Loss: 59.51038360595703\n",
      "Epoch 0: Loss: 65.27778625488281\n",
      "Epoch 0: Loss: 56.52922821044922\n",
      "Epoch 0: Loss: 80.50812530517578\n",
      "Epoch 0: Loss: 90.45880889892578\n",
      "Epoch 0: Loss: 97.77892303466797\n",
      "Epoch 0: Loss: 73.45755004882812\n",
      "Epoch 0: Loss: 66.55362701416016\n",
      "Epoch 0: Loss: 63.07869338989258\n",
      "Epoch 0: Loss: 67.32199096679688\n",
      "Epoch 0: Loss: 66.3425521850586\n",
      "Epoch 0: Loss: 69.37852478027344\n",
      "Epoch 0: Loss: 90.40641784667969\n",
      "Epoch 0: Loss: 83.37910461425781\n",
      "Epoch 0: Loss: 60.450286865234375\n",
      "Epoch 0: Loss: 70.29672241210938\n",
      "Epoch 0: Loss: 74.19551086425781\n",
      "Epoch 0: Loss: 54.360496520996094\n",
      "Epoch 0: Loss: 62.47615432739258\n",
      "Epoch 0: Loss: 85.56068420410156\n",
      "Epoch 0: Loss: 82.16481018066406\n",
      "Epoch 0: Loss: 76.48724365234375\n",
      "Epoch 0: Loss: 98.19937896728516\n",
      "Epoch 0: Loss: 97.35105895996094\n",
      "Epoch 0: Loss: 66.50527954101562\n",
      "Epoch 0: Loss: 93.8335189819336\n",
      "Epoch 0: Loss: 81.34959411621094\n",
      "Epoch 0: Loss: 88.93399810791016\n",
      "Epoch 0: Loss: 83.61983489990234\n",
      "Epoch 0: Loss: 76.25556945800781\n",
      "Epoch 0: Loss: 77.58145141601562\n",
      "Epoch 0: Loss: 81.46813201904297\n",
      "Epoch 0: Loss: 79.27832794189453\n",
      "Epoch 0: Loss: 71.59561157226562\n",
      "Epoch 0: Loss: 55.15351104736328\n",
      "Epoch 0: Loss: 67.45592498779297\n",
      "Epoch 0: Loss: 61.56517028808594\n",
      "Epoch 0: Loss: 80.558349609375\n",
      "Epoch 0: Loss: 72.24050903320312\n",
      "Epoch 0: Loss: 89.77659606933594\n",
      "Epoch 0: Loss: 86.58018493652344\n",
      "Epoch 0: Loss: 78.77757263183594\n",
      "Epoch 0: Loss: 101.18334197998047\n",
      "Epoch 0: Loss: 85.40374755859375\n",
      "Epoch 0: Loss: 78.40220642089844\n",
      "Epoch 0: Loss: 80.23127746582031\n",
      "Epoch 0: Loss: 73.37001037597656\n",
      "Epoch 0: Loss: 67.26264190673828\n",
      "Epoch 0: Loss: 71.27515411376953\n",
      "Epoch 0: Loss: 86.14491271972656\n",
      "Epoch 0: Loss: 67.70303344726562\n",
      "Epoch 0: Loss: 72.73091888427734\n",
      "Epoch 0: Loss: 98.51824951171875\n",
      "Epoch 0: Loss: 70.78120422363281\n",
      "Epoch 0: Loss: 64.62748718261719\n",
      "Epoch 0: Loss: 74.59112548828125\n",
      "Epoch 0: Loss: 58.485660552978516\n",
      "Epoch 0: Loss: 66.45873260498047\n",
      "Epoch 0: Loss: 63.2722053527832\n",
      "Epoch 0: Loss: 57.5037956237793\n",
      "Epoch 0: Loss: 65.59098052978516\n",
      "Epoch 0: Loss: 51.53376770019531\n",
      "Epoch 0: Loss: 68.66716766357422\n",
      "Epoch 0: Loss: 50.611019134521484\n",
      "Epoch 0: Loss: 49.33807373046875\n",
      "Epoch 0: Loss: 66.31668853759766\n",
      "Epoch 0: Loss: 65.2950668334961\n",
      "Epoch 0: Loss: 66.26728820800781\n",
      "Epoch 0: Loss: 75.94598388671875\n",
      "Epoch 0: Loss: 83.57901000976562\n",
      "Epoch 0: Loss: 93.38418579101562\n",
      "Epoch 0: Loss: 83.23241424560547\n",
      "Epoch 0: Loss: 63.47783279418945\n",
      "Epoch 0: Loss: 85.57890319824219\n",
      "Epoch 0: Loss: 81.4482421875\n",
      "Epoch 0: Loss: 76.1240463256836\n",
      "Epoch 0: Loss: 60.378692626953125\n",
      "Epoch 0: Loss: 79.48663330078125\n",
      "Epoch 0: Loss: 58.33522415161133\n",
      "Epoch 0: Loss: 49.064109802246094\n",
      "Epoch 0: Loss: 53.525508880615234\n",
      "Epoch 0: Loss: 48.41383361816406\n",
      "Epoch 0: Loss: 59.48863220214844\n",
      "Epoch 0: Loss: 59.65514373779297\n",
      "Epoch 0: Loss: 95.85589599609375\n",
      "Epoch 0: Loss: 73.63455200195312\n",
      "Epoch 0: Loss: 82.48518371582031\n",
      "Epoch 0: Loss: 66.47129821777344\n",
      "Epoch 0: Loss: 63.64641571044922\n",
      "Epoch 0: Loss: 46.300880432128906\n",
      "Epoch 0: Loss: 44.516632080078125\n",
      "Epoch 0: Loss: 48.38910675048828\n",
      "Epoch 0: Loss: 52.38808822631836\n",
      "Epoch 0: Loss: 63.48324203491211\n",
      "Epoch 0: Loss: 63.295997619628906\n",
      "Epoch 0: Loss: 57.22886657714844\n",
      "Epoch 0: Loss: 75.52610778808594\n",
      "Epoch 0: Loss: 58.277244567871094\n",
      "Epoch 0: Loss: 64.39352416992188\n",
      "Epoch 0: Loss: 70.30654907226562\n",
      "Epoch 0: Loss: 63.423561096191406\n",
      "Epoch 0: Loss: 69.42724609375\n",
      "Epoch 0: Loss: 70.03583526611328\n",
      "Epoch 0: Loss: 72.5202865600586\n",
      "Epoch 0: Loss: 75.47988891601562\n",
      "Epoch 0: Loss: 51.41403579711914\n",
      "Epoch 0: Loss: 71.52204132080078\n",
      "Epoch 0: Loss: 69.1880874633789\n",
      "Epoch 0: Loss: 68.08028411865234\n",
      "Epoch 0: Loss: 55.14802932739258\n",
      "Epoch 0: Loss: 76.3002700805664\n",
      "Epoch 0: Loss: 67.1818618774414\n",
      "Epoch 0: Loss: 53.29528045654297\n",
      "Epoch 0: Loss: 53.2734489440918\n",
      "Epoch 0: Loss: 60.33934783935547\n",
      "Epoch 0: Loss: 62.332122802734375\n",
      "Epoch 0: Loss: 62.334739685058594\n",
      "Epoch 0: Loss: 61.4622802734375\n",
      "Epoch 0: Loss: 77.49666595458984\n",
      "Epoch 0: Loss: 64.27129364013672\n",
      "Epoch 0: Loss: 46.28334045410156\n",
      "Epoch 0: Loss: 43.410911560058594\n",
      "Epoch 0: Loss: 55.46416473388672\n",
      "Epoch 0: Loss: 55.54801940917969\n",
      "Epoch 0: Loss: 68.28782653808594\n",
      "Epoch 0: Loss: 78.099365234375\n",
      "Epoch 0: Loss: 83.80268859863281\n",
      "Epoch 0: Loss: 74.59393310546875\n",
      "Epoch 0: Loss: 66.60025787353516\n",
      "Epoch 0: Loss: 67.18617248535156\n",
      "Epoch 0: Loss: 59.0927619934082\n",
      "Epoch 0: Loss: 65.08746337890625\n",
      "Epoch 0: Loss: 57.256683349609375\n",
      "Epoch 0: Loss: 67.21223449707031\n",
      "Epoch 0: Loss: 76.78323364257812\n",
      "Epoch 0: Loss: 70.67904663085938\n",
      "Epoch 0: Loss: 67.86182403564453\n",
      "Epoch 0: Loss: 70.60877990722656\n",
      "Epoch 0: Loss: 56.262001037597656\n",
      "Epoch 0: Loss: 45.30950164794922\n",
      "Epoch 0: Loss: 65.90653991699219\n",
      "Epoch 0: Loss: 69.58680725097656\n",
      "Epoch 0: Loss: 47.32288360595703\n",
      "Epoch 0: Loss: 58.452476501464844\n",
      "Epoch 0: Loss: 64.57823181152344\n",
      "Epoch 0: Loss: 47.29906463623047\n",
      "Epoch 0: Loss: 62.70709228515625\n",
      "Epoch 0: Loss: 73.991455078125\n",
      "Epoch 0: Loss: 42.16131591796875\n",
      "Epoch 0: Loss: 63.937713623046875\n",
      "Epoch 0: Loss: 54.26783752441406\n",
      "Epoch 0: Loss: 48.276092529296875\n",
      "Epoch 0: Loss: 43.43994140625\n",
      "Epoch 0: Loss: 41.3247184753418\n",
      "Epoch 0: Loss: 58.48688507080078\n",
      "Epoch 0: Loss: 50.31289291381836\n",
      "Epoch 0: Loss: 58.31665802001953\n",
      "Epoch 0: Loss: 49.258052825927734\n",
      "Epoch 0: Loss: 46.22312545776367\n",
      "Epoch 0: Loss: 56.25019836425781\n",
      "Epoch 0: Loss: 44.27172088623047\n",
      "Epoch 0: Loss: 55.23692321777344\n",
      "Epoch 0: Loss: 53.21238708496094\n",
      "Epoch 0: Loss: 72.17864227294922\n",
      "Epoch 0: Loss: 70.04633331298828\n",
      "Epoch 0: Loss: 77.016845703125\n",
      "Epoch 0: Loss: 71.1710205078125\n",
      "Epoch 0: Loss: 84.31883239746094\n",
      "Epoch 0: Loss: 88.16606140136719\n",
      "Epoch 0: Loss: 92.32272338867188\n",
      "Epoch 0: Loss: 88.12601470947266\n",
      "Epoch 0: Loss: 82.79426574707031\n",
      "Epoch 0: Loss: 71.25646209716797\n",
      "Epoch 0: Loss: 68.51311492919922\n",
      "Epoch 0: Loss: 49.385562896728516\n",
      "Epoch 0: Loss: 52.053245544433594\n",
      "Epoch 0: Loss: 48.17921829223633\n",
      "Epoch 0: Loss: 71.5429458618164\n",
      "Epoch 0: Loss: 80.5097427368164\n",
      "Epoch 0: Loss: 69.40560913085938\n",
      "Epoch 0: Loss: 61.20999526977539\n",
      "Epoch 0: Loss: 58.169620513916016\n",
      "Epoch 0: Loss: 73.43331146240234\n",
      "Epoch 0: Loss: 79.16592407226562\n",
      "Epoch 0: Loss: 82.21598052978516\n",
      "Epoch 0: Loss: 74.51585388183594\n",
      "Epoch 0: Loss: 77.35298156738281\n",
      "Epoch 0: Loss: 64.54428100585938\n",
      "Epoch 0: Loss: 49.3770866394043\n",
      "Epoch 0: Loss: 56.19938278198242\n",
      "Epoch 0: Loss: 59.299591064453125\n",
      "Epoch 0: Loss: 65.19723510742188\n",
      "Epoch 0: Loss: 60.40217590332031\n",
      "Epoch 0: Loss: 49.13283920288086\n",
      "Epoch 0: Loss: 52.38768768310547\n",
      "Epoch 0: Loss: 68.18927001953125\n",
      "Epoch 0: Loss: 76.25515747070312\n",
      "Epoch 0: Loss: 75.19551849365234\n",
      "Epoch 0: Loss: 67.35566711425781\n",
      "Epoch 0: Loss: 81.4700698852539\n",
      "Epoch 0: Loss: 53.35397720336914\n",
      "Epoch 0: Loss: 57.49909591674805\n",
      "Epoch 0: Loss: 62.42747497558594\n",
      "Epoch 0: Loss: 51.476722717285156\n",
      "Epoch 0: Loss: 49.11407470703125\n",
      "Epoch 0: Loss: 71.26535034179688\n",
      "Epoch 0: Loss: 80.34030151367188\n",
      "Epoch 0: Loss: 81.13492584228516\n",
      "Epoch 0: Loss: 100.17816162109375\n",
      "Epoch 0: Loss: 92.74292755126953\n",
      "Epoch 0: Loss: 55.27708053588867\n",
      "Epoch 0: Loss: 58.40338134765625\n",
      "Epoch 0: Loss: 61.26372528076172\n",
      "Epoch 0: Loss: 64.15396881103516\n",
      "Epoch 0: Loss: 61.353858947753906\n",
      "Epoch 0: Loss: 51.169952392578125\n",
      "Epoch 0: Loss: 57.053890228271484\n",
      "Epoch 0: Loss: 45.24589538574219\n",
      "Epoch 0: Loss: 45.2122917175293\n",
      "Epoch 0: Loss: 52.135047912597656\n",
      "Epoch 0: Loss: 53.25799560546875\n",
      "Epoch 0: Loss: 54.213279724121094\n",
      "Epoch 0: Loss: 68.31005096435547\n",
      "Epoch 0: Loss: 59.159584045410156\n",
      "Epoch 0: Loss: 52.22650146484375\n",
      "Epoch 0: Loss: 56.31652069091797\n",
      "Epoch 0: Loss: 39.10325622558594\n",
      "Epoch 0: Loss: 53.12898254394531\n",
      "Epoch 0: Loss: 49.154293060302734\n",
      "Epoch 0: Loss: 44.30341339111328\n",
      "Epoch 0: Loss: 57.28466796875\n",
      "Epoch 0: Loss: 56.24699783325195\n",
      "Epoch 0: Loss: 66.24029541015625\n",
      "Epoch 0: Loss: 54.22560119628906\n",
      "Epoch 0: Loss: 51.29084777832031\n",
      "Epoch 0: Loss: 47.09392547607422\n",
      "Epoch 0: Loss: 61.44548416137695\n",
      "Epoch 0: Loss: 59.28605651855469\n",
      "Epoch 0: Loss: 57.25251388549805\n",
      "Epoch 0: Loss: 45.486820220947266\n",
      "Epoch 0: Loss: 53.112335205078125\n",
      "Epoch 0: Loss: 59.06476593017578\n",
      "Epoch 0: Loss: 78.23868560791016\n",
      "Epoch 0: Loss: 67.37068939208984\n",
      "Epoch 0: Loss: 64.03396606445312\n",
      "Epoch 0: Loss: 65.86315155029297\n",
      "Epoch 0: Loss: 74.70187377929688\n",
      "Epoch 0: Loss: 71.53974914550781\n",
      "Epoch 0: Loss: 64.53016662597656\n",
      "Epoch 0: Loss: 65.23200988769531\n",
      "Epoch 0: Loss: 70.2265625\n",
      "Epoch 0: Loss: 64.28913879394531\n",
      "Epoch 0: Loss: 81.28547668457031\n",
      "Epoch 0: Loss: 68.25027465820312\n",
      "Epoch 0: Loss: 66.04512786865234\n",
      "Epoch 0: Loss: 58.167449951171875\n",
      "Epoch 0: Loss: 59.376708984375\n",
      "Epoch 0: Loss: 65.35778045654297\n",
      "Epoch 0: Loss: 60.32429122924805\n",
      "Epoch 0: Loss: 59.14939880371094\n",
      "Epoch 0: Loss: 78.43807983398438\n",
      "Epoch 0: Loss: 57.22953796386719\n",
      "Epoch 0: Loss: 49.21039581298828\n",
      "Epoch 0: Loss: 49.46205520629883\n",
      "Epoch 0: Loss: 52.30864715576172\n",
      "Epoch 0: Loss: 45.17646026611328\n",
      "Epoch 0: Loss: 48.25086212158203\n",
      "Epoch 0: Loss: 51.09230041503906\n",
      "Epoch 0: Loss: 54.382057189941406\n",
      "Epoch 0: Loss: 57.20077133178711\n",
      "Epoch 0: Loss: 70.20924377441406\n",
      "Epoch 0: Loss: 48.13618469238281\n",
      "Epoch 0: Loss: 57.207130432128906\n",
      "Epoch 0: Loss: 51.521629333496094\n",
      "Epoch 0: Loss: 59.43986892700195\n",
      "Epoch 0: Loss: 58.366546630859375\n",
      "Epoch 0: Loss: 54.685665130615234\n",
      "Epoch 0: Loss: 47.2259521484375\n",
      "Epoch 0: Loss: 42.143882751464844\n",
      "Epoch 0: Loss: 56.99323272705078\n",
      "Epoch 0: Loss: 36.080810546875\n",
      "Epoch 0: Loss: 40.083473205566406\n",
      "Epoch 0: Loss: 52.991512298583984\n",
      "Epoch 0: Loss: 39.17887878417969\n",
      "Epoch 0: Loss: 52.09204864501953\n",
      "Epoch 0: Loss: 39.08354949951172\n",
      "Epoch 0: Loss: 53.211238861083984\n",
      "Epoch 0: Loss: 49.151405334472656\n",
      "Epoch 0: Loss: 47.352413177490234\n",
      "Epoch 0: Loss: 49.1575927734375\n",
      "Epoch 0: Loss: 51.29879379272461\n",
      "Epoch 0: Loss: 60.470611572265625\n",
      "Epoch 0: Loss: 46.18745422363281\n",
      "Epoch 0: Loss: 50.21010971069336\n",
      "Epoch 0: Loss: 54.32102966308594\n",
      "Epoch 0: Loss: 53.115325927734375\n",
      "Epoch 0: Loss: 59.08245086669922\n",
      "Epoch 0: Loss: 52.228981018066406\n",
      "Epoch 0: Loss: 66.26731872558594\n",
      "Epoch 0: Loss: 53.43743896484375\n",
      "Epoch 0: Loss: 68.25431823730469\n",
      "Epoch 0: Loss: 50.58820343017578\n",
      "Epoch 0: Loss: 69.6887435913086\n",
      "Epoch 0: Loss: 83.40245819091797\n",
      "Epoch 0: Loss: 66.62040710449219\n",
      "Epoch 0: Loss: 52.223228454589844\n",
      "Epoch 0: Loss: 59.286529541015625\n",
      "Epoch 0: Loss: 58.168060302734375\n",
      "Epoch 0: Loss: 56.0709228515625\n",
      "Epoch 0: Loss: 58.055999755859375\n",
      "Epoch 0: Loss: 68.28043365478516\n",
      "Epoch 0: Loss: 56.23326110839844\n",
      "Epoch 0: Loss: 51.451725006103516\n",
      "Epoch 0: Loss: 54.322120666503906\n",
      "Epoch 0: Loss: 59.5478630065918\n",
      "Epoch 0: Loss: 53.99809265136719\n",
      "Epoch 0: Loss: 50.56482696533203\n",
      "Epoch 0: Loss: 56.21272277832031\n",
      "Epoch 0: Loss: 44.330108642578125\n",
      "Epoch 0: Loss: 55.92673110961914\n",
      "Epoch 0: Loss: 58.14619064331055\n",
      "Epoch 0: Loss: 70.0295639038086\n",
      "Epoch 0: Loss: 60.41228485107422\n",
      "Epoch 0: Loss: 68.14350891113281\n",
      "Epoch 0: Loss: 63.04121398925781\n",
      "Epoch 0: Loss: 65.18190002441406\n",
      "Epoch 0: Loss: 53.961605072021484\n",
      "Epoch 0: Loss: 56.15053939819336\n",
      "Epoch 0: Loss: 52.248748779296875\n",
      "Epoch 0: Loss: 60.984649658203125\n",
      "Epoch 0: Loss: 51.19538116455078\n",
      "Epoch 0: Loss: 70.41233825683594\n",
      "Epoch 0: Loss: 52.08831787109375\n",
      "Epoch 0: Loss: 62.165931701660156\n",
      "Epoch 0: Loss: 52.182464599609375\n",
      "Epoch 0: Loss: 56.170738220214844\n",
      "Epoch 0: Loss: 38.12458801269531\n",
      "Epoch 0: Loss: 57.05524444580078\n",
      "Epoch 0: Loss: 46.966575622558594\n",
      "Epoch 0: Loss: 52.22084045410156\n",
      "Epoch 0: Loss: 65.09028625488281\n",
      "Epoch 0: Loss: 59.982357025146484\n",
      "Epoch 0: Loss: 47.066558837890625\n",
      "Epoch 0: Loss: 55.06333923339844\n",
      "Epoch 0: Loss: 44.137542724609375\n",
      "Epoch 0: Loss: 43.115779876708984\n",
      "Epoch 0: Loss: 57.20869064331055\n",
      "Epoch 0: Loss: 44.39649963378906\n",
      "Epoch 0: Loss: 53.307708740234375\n",
      "Epoch 0: Loss: 46.949100494384766\n",
      "Epoch 0: Loss: 55.00717544555664\n",
      "Epoch 0: Loss: 44.96396255493164\n",
      "Epoch 0: Loss: 51.704463958740234\n",
      "Epoch 0: Loss: 84.72343444824219\n",
      "Epoch 0: Loss: 76.524169921875\n",
      "Epoch 0: Loss: 78.7589111328125\n",
      "Epoch 0: Loss: 91.2916259765625\n",
      "Epoch 0: Loss: 83.36170959472656\n",
      "Epoch 0: Loss: 55.60480880737305\n",
      "Epoch 0: Loss: 55.597190856933594\n",
      "Epoch 0: Loss: 84.24633026123047\n",
      "Epoch 0: Loss: 68.043212890625\n",
      "Epoch 0: Loss: 74.00357055664062\n",
      "Epoch 0: Loss: 72.4446029663086\n",
      "Epoch 0: Loss: 66.40509033203125\n",
      "Epoch 0: Loss: 44.27120590209961\n",
      "Epoch 0: Loss: 56.509193420410156\n",
      "Epoch 0: Loss: 54.28863525390625\n",
      "Epoch 0: Loss: 74.19904327392578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-c7c9b88686a9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mtime\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEPISODE_LENGTH\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[0mobs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m         \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrand\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mEPSILON\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mepisode\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mWARMUP_LENGTH\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m             \u001B[0mnext_action\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction_space\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\leeji\\pycharmprojects\\algorithms\\venv\\lib\\site-packages\\gym\\core.py\u001B[0m in \u001B[0;36mrender\u001B[1;34m(self, mode, **kwargs)\u001B[0m\n\u001B[0;32m    238\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    239\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'human'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 240\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    241\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\leeji\\pycharmprojects\\algorithms\\venv\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001B[0m in \u001B[0;36mrender\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    211\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpoletrans\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_rotation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 213\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mviewer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreturn_rgb_array\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'rgb_array'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    214\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    215\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\leeji\\pycharmprojects\\algorithms\\venv\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001B[0m in \u001B[0;36mrender\u001B[1;34m(self, return_rgb_array)\u001B[0m\n\u001B[0;32m    125\u001B[0m             \u001B[0marr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbuffer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mheight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwidth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    126\u001B[0m             \u001B[0marr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 127\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwindow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    128\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0monetime_geoms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0marr\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mreturn_rgb_array\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misopen\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\leeji\\pycharmprojects\\algorithms\\venv\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001B[0m in \u001B[0;36mflip\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    334\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_always_dwm\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dwm_composition_enabled\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    335\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_interval\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 336\u001B[1;33m                     \u001B[0m_dwmapi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDwmFlush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    337\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    338\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODE_NUM):\n",
    "    env.reset()\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    for time in range(EPISODE_LENGTH):\n",
    "        obs = torch.from_numpy(obs)\n",
    "        env.render()\n",
    "        if np.random.rand() < EPSILON or episode < WARMUP_LENGTH:\n",
    "            next_action = env.action_space.sample()\n",
    "        else:\n",
    "            action_values = model(torch.unsqueeze(obs, 0).type(torch.FloatTensor).cuda())[0]\n",
    "            next_action = torch.argmax(action_values).cpu().numpy()\n",
    "        obs2, reward, done, info = env.step(next_action)\n",
    "        replay.add_one((obs, next_action, reward, obs2))\n",
    "        obs = obs2\n",
    "    if episode >= WARMUP_LENGTH:\n",
    "        obs_arr, action_arr, reward_arr, obs2_arr = replay.sample(MINIBATCH_SIZE)\n",
    "        idx = torch.tensor(range(MINIBATCH_SIZE), dtype=torch.long)\n",
    "        x = obs_arr.type(torch.FloatTensor).cuda()\n",
    "        y = model(x)\n",
    "        y2 = model(obs2_arr.type(torch.FloatTensor).cuda())\n",
    "        action_arr = action_arr.to(dtype=torch.long)\n",
    "        y[idx, action_arr] = GAMMA * torch.max(y2, dim=1)[0] + reward_arr.type(torch.FloatTensor).cuda()\n",
    "        fit(EPOCHS, MINIBATCH_SIZE, LEARNING_RATE, model, loss_func, x, y)\n",
    "torch.save(model, 'deep_gym_model.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}